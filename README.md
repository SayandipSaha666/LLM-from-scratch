# LLM-from-scratch
This project implements a GPT-2-style transformer language model entirely from the ground up using PyTorch. It covers every stage — from data preprocessing and tokenization to building the multi-head attention mechanism, transformer blocks, and training loop — without relying on high-level libraries. The goal is to deeply understand how large language models like GPT-2 work internally, including:

Implementing Byte Pair Encoding (BPE) tokenization

Designing positional embeddings and multi-head self-attention

Stacking transformer decoder blocks to form the model

Training on text datasets and generating coherent text samples

This repository serves as an educational resource for anyone who wants to explore the inner workings of modern language models at the code level.
